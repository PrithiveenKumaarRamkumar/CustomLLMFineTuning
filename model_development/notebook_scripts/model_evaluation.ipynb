{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aJWQUp82Nzn0jttZJ8o8CnvnoBSrG1-q","timestamp":1764049938520}],"gpuType":"T4","mount_file_id":"1aJWQUp82Nzn0jttZJ8o8CnvnoBSrG1-q","authorship_tag":"ABX9TyOgFyBl6x/FS5cyT6u81iEw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"638e80e4667b433088a39fdb1356d19b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ca8bce06d9c4fe9a591623fb803b984","IPY_MODEL_e3de5fe9693144faa33fd8262ff1749e","IPY_MODEL_f0312fc63016475b86cf933bb870b50d"],"layout":"IPY_MODEL_6b393112d49c4d49a2d727518c47d3fd"}},"5ca8bce06d9c4fe9a591623fb803b984":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cbde2259fd14a9db24ff604ed5e077f","placeholder":"​","style":"IPY_MODEL_06e76a545ffc4883be4811b738b7cc25","value":"Generating train split: "}},"e3de5fe9693144faa33fd8262ff1749e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_493aa763500b4810955a0f8716c35843","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_892325382abe46d2a9120bf21cb5c21d","value":1}},"f0312fc63016475b86cf933bb870b50d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02460803174f4e4b844fd9eef60daa49","placeholder":"​","style":"IPY_MODEL_4bfdab0c7d8f47149eff6f4748aae7e6","value":" 42/0 [00:00&lt;00:00, 327.99 examples/s]"}},"6b393112d49c4d49a2d727518c47d3fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cbde2259fd14a9db24ff604ed5e077f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06e76a545ffc4883be4811b738b7cc25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"493aa763500b4810955a0f8716c35843":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"892325382abe46d2a9120bf21cb5c21d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"02460803174f4e4b844fd9eef60daa49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bfdab0c7d8f47149eff6f4748aae7e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# ============================================================\n","# Create Evaluation Dataset from Training Data\n","# Splits code into prompt and reference for evaluation\n","# ============================================================\n","\n","import json\n","from datasets import load_dataset\n","import random\n","\n","def create_eval_dataset_from_training(\n","    training_dataset_path=\"/content/drive/MyDrive/code_dataset.json\",\n","    output_path=\"/content/eval_dataset.json\",\n","    test_split=0.2,\n","    max_samples=None\n","):\n","    \"\"\"\n","    Create evaluation dataset from training data\n","    Extracts prompts and references from code samples\n","    \"\"\"\n","\n","    print(\"=\"*60)\n","    print(\"CREATING EVALUATION DATASET\")\n","    print(\"=\"*60 + \"\\n\")\n","\n","    # Load training dataset\n","    print(f\"Loading training data from {training_dataset_path}...\")\n","    dataset = load_dataset(\"json\", data_files=training_dataset_path, split=\"train\")\n","    print(f\"✓ Loaded {len(dataset)} samples\\n\")\n","\n","    # Create eval samples\n","    eval_samples = []\n","\n","    for item in dataset:\n","        # Get the code text\n","        if \"text\" in item:\n","            code = item[\"text\"]\n","            language = item.get(\"language\", \"python\")\n","        elif \"output\" in item:\n","            code = item[\"output\"]\n","            language = item.get(\"language\", \"python\")\n","        else:\n","            continue\n","\n","        # Extract prompt and reference\n","        prompt, reference = extract_prompt_and_reference(code, language)\n","\n","        if prompt and reference:\n","            eval_samples.append({\n","                \"prompt\": prompt,\n","                \"reference\": reference,\n","                \"language\": language,\n","                \"file_name\": item.get(\"file_name\", \"unknown\")\n","            })\n","\n","    print(f\"✓ Created {len(eval_samples)} evaluation samples\\n\")\n","\n","    # Shuffle and limit\n","    random.shuffle(eval_samples)\n","\n","    if max_samples:\n","        eval_samples = eval_samples[:max_samples]\n","        print(f\"Limited to {max_samples} samples\")\n","\n","    # Save\n","    with open(output_path, \"w\") as f:\n","        json.dump(eval_samples, f, indent=2)\n","\n","    print(f\"\\n✓ Evaluation dataset saved to {output_path}\")\n","    print(f\"Total samples: {len(eval_samples)}\")\n","\n","    # Print statistics\n","    print(\"\\nDataset Statistics:\")\n","    print(\"-\" * 60)\n","    languages = {}\n","    for sample in eval_samples:\n","        lang = sample[\"language\"]\n","        languages[lang] = languages.get(lang, 0) + 1\n","\n","    for lang, count in sorted(languages.items()):\n","        print(f\"  {lang}: {count} samples\")\n","\n","    return eval_samples\n","\n","def extract_prompt_and_reference(code, language=\"python\"):\n","    \"\"\"\n","    Extract prompt and reference from code\n","\n","    Strategies:\n","    1. For functions: Use signature as prompt, full code as reference\n","    2. For classes: Use class declaration as prompt, full code as reference\n","    3. For other code: Use first line as prompt, full code as reference\n","    \"\"\"\n","\n","    lines = code.strip().split('\\n')\n","\n","    if not lines:\n","        return None, None\n","\n","    if language == \"python\":\n","        # Function definition\n","        if lines[0].strip().startswith(\"def \"):\n","            prompt = lines[0].strip()\n","            reference = code\n","            return prompt, reference\n","\n","        # Class definition\n","        elif lines[0].strip().startswith(\"class \"):\n","            prompt = lines[0].strip()\n","            reference = code\n","            return prompt, reference\n","\n","        # Import or other\n","        else:\n","            # Use first significant line as prompt\n","            for line in lines:\n","                if line.strip() and not line.strip().startswith(\"#\"):\n","                    prompt = line.strip()\n","                    reference = code\n","                    return prompt, reference\n","\n","    elif language == \"javascript\" or language == \"ts\":\n","        # Function definition\n","        if \"function \" in lines[0] or \"=>\" in lines[0] or \"const \" in lines[0]:\n","            prompt = lines[0].strip()\n","            reference = code\n","            return prompt, reference\n","\n","        # Class definition\n","        elif lines[0].strip().startswith(\"class \"):\n","            prompt = lines[0].strip()\n","            reference = code\n","            return prompt, reference\n","\n","    elif language == \"java\":\n","        # Method or class\n","        for i, line in enumerate(lines):\n","            if \"public \" in line or \"private \" in line or \"protected \" in line:\n","                if \"class \" in line or \"interface \" in line:\n","                    prompt = line.strip()\n","                    reference = code\n","                    return prompt, reference\n","                elif \"(\" in line:  # Method\n","                    prompt = line.strip()\n","                    reference = code\n","                    return prompt, reference\n","\n","    elif language == \"cpp\":\n","        # Function definition\n","        for line in lines:\n","            if \"(\" in line and \"{\" not in line:\n","                prompt = line.strip()\n","                reference = code\n","                return prompt, reference\n","\n","    # Default: use first line as prompt\n","    prompt = lines[0].strip()\n","    reference = code\n","    return prompt, reference\n","\n","def split_train_eval(\n","    dataset_path=\"code_dataset.json\",\n","    train_output=\"train_dataset.json\",\n","    eval_output=\"eval_dataset.json\",\n","    test_split=0.2\n","):\n","    \"\"\"\n","    Split existing dataset into train and eval sets\n","    \"\"\"\n","\n","    print(\"=\"*60)\n","    print(\"SPLITTING DATASET INTO TRAIN/EVAL\")\n","    print(\"=\"*60 + \"\\n\")\n","\n","    # Load dataset\n","    print(f\"Loading dataset from {dataset_path}...\")\n","    with open(dataset_path, \"r\") as f:\n","        data = json.load(f)\n","\n","    print(f\"✓ Loaded {len(data)} samples\\n\")\n","\n","    # Shuffle\n","    random.shuffle(data)\n","\n","    # Split\n","    split_idx = int(len(data) * (1 - test_split))\n","    train_data = data[:split_idx]\n","    eval_data = data[split_idx:]\n","\n","    # Save\n","    with open(train_output, \"w\") as f:\n","        json.dump(train_data, f, indent=2)\n","\n","    with open(eval_output, \"w\") as f:\n","        json.dump(eval_data, f, indent=2)\n","\n","    print(f\"✓ Train set saved to {train_output} ({len(train_data)} samples)\")\n","    print(f\"✓ Eval set saved to {eval_output} ({len(eval_data)} samples)\")\n","    print(f\"\\nSplit ratio: {(1-test_split)*100:.0f}% train, {test_split*100:.0f}% eval\")\n","\n","# ============================================================\n","# EXAMPLE USAGE\n","# ============================================================\n","if __name__ == \"__main__\":\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"OPTION 1: Create eval dataset with prompts extracted\")\n","    print(\"=\"*60)\n","\n","    eval_samples = create_eval_dataset_from_training(\n","        training_dataset_path=\"/content/drive/MyDrive/code_dataset.json\",\n","        output_path=\"eval_dataset.json\",\n","        max_samples=100  # Limit to 100 samples for faster evaluation\n","    )\n","\n","    # Show examples\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"SAMPLE EVALUATION ENTRIES\")\n","    print(\"=\"*60)\n","\n","    for i, sample in enumerate(eval_samples[:3]):\n","        print(f\"\\nSample {i+1}:\")\n","        print(f\"Language: {sample['language']}\")\n","        print(f\"Prompt: {sample['prompt']}\")\n","        print(f\"Reference (first 100 chars): {sample['reference'][:100]}...\")\n","        print(\"-\" * 60)\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"OPTION 2: Simple train/eval split\")\n","    print(\"=\"*60)\n","\n","    # Uncomment to use simple split instead\n","    # split_train_eval(\n","    #     dataset_path=\"code_dataset.json\",\n","    #     train_output=\"train_dataset.json\",\n","    #     eval_output=\"eval_dataset.json\",\n","    #     test_split=0.2\n","    # )\n","\n","    print(\"\\n✓ Dataset preparation complete!\")\n","    print(\"\\nNext step: Run the evaluation script:\")\n","    print(\"  python evaluate_model.py\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lV2WVRvDaJ_g","executionInfo":{"status":"ok","timestamp":1764047316962,"user_tz":300,"elapsed":315,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}},"outputId":"830d866a-797f-4e36-df78-fc561420af71"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","OPTION 1: Create eval dataset with prompts extracted\n","============================================================\n","============================================================\n","CREATING EVALUATION DATASET\n","============================================================\n","\n","Loading training data from /content/drive/MyDrive/code_dataset.json...\n","✓ Loaded 42 samples\n","\n","✓ Created 42 evaluation samples\n","\n","Limited to 100 samples\n","\n","✓ Evaluation dataset saved to eval_dataset.json\n","Total samples: 42\n","\n","Dataset Statistics:\n","------------------------------------------------------------\n","  cpp: 42 samples\n","\n","============================================================\n","SAMPLE EVALUATION ENTRIES\n","============================================================\n","\n","Sample 1:\n","Language: cpp\n","Prompt: UNIT_TEST(Assert_Smoke)\n","Reference (first 100 chars): #include \"testing/testing.hpp\"\n","\n","#include \"base/base.hpp\"\n","#include \"base/exception.hpp\"\n","#include \"bas...\n","------------------------------------------------------------\n","\n","Sample 2:\n","Language: cpp\n","Prompt: * Copyright (c) 2004-present, The University of Notre Dame. All rights\n","Reference (first 100 chars): /*\n"," * Copyright (c) 2004-present, The University of Notre Dame. All rights\n"," * reserved.\n"," *\n"," * Redist...\n","------------------------------------------------------------\n","\n","Sample 3:\n","Language: cpp\n","Prompt: // Copyright (c) 2013-2020 Baptiste Wicht.\n","Reference (first 100 chars): //===\n","// Copyright (c) 2013-2020 Baptiste Wicht.\n","// Distributed under the terms of the MIT License.\n","...\n","------------------------------------------------------------\n","\n","============================================================\n","OPTION 2: Simple train/eval split\n","============================================================\n","\n","✓ Dataset preparation complete!\n","\n","Next step: Run the evaluation script:\n","  python evaluate_model.py\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# Comprehensive Model Evaluation for Code Generation\n","# Metrics: CodeBLEU, BLEU, Exact Match, Pass@k, etc.\n","# ============================================================\n","\n","# Install dependencies\n","import subprocess\n","import sys\n","\n","def install_packages():\n","    packages = [\n","        \"transformers\",\n","        \"datasets\",\n","        \"peft\",\n","        \"torch\",\n","        \"evaluate\",\n","        \"sacrebleu\",\n","        \"codebleu\",\n","        \"tree-sitter\",\n","        \"tree-sitter-python\",\n","        \"tree-sitter-java\",\n","        \"tree-sitter-javascript\",\n","        \"tree-sitter-cpp\",\n","        \"tree-sitter-c-sharp\",\n","        \"tree-sitter-go\",\n","        \"nltk\",\n","        \"rouge_score\" # Added rouge_score dependency\n","    ]\n","\n","    print(\"Installing evaluation packages...\")\n","    for package in packages:\n","        subprocess.run(\n","            [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package],\n","            stdout=subprocess.DEVNULL,\n","            stderr=subprocess.DEVNULL\n","        )\n","    print(\"✓ Packages installed\\n\")\n","\n","install_packages()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NS-Rtw4BeYZU","executionInfo":{"status":"ok","timestamp":1764049201939,"user_tz":300,"elapsed":47359,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}},"outputId":"7bf9ea61-58bd-4d92-85c1-03fe87d8a261"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing evaluation packages...\n","✓ Packages installed\n","\n"]}]},{"cell_type":"code","source":["# Imports\n","import torch\n","import json\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel\n","from datasets import load_dataset\n","from codebleu import calc_codebleu\n","import evaluate\n","from tqdm import tqdm\n","import numpy as np\n","from collections import defaultdict\n","import time"],"metadata":{"id":"8kY1NwmVepWq","executionInfo":{"status":"ok","timestamp":1764048771128,"user_tz":300,"elapsed":34852,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# LOAD MODEL\n","# ============================================================\n","def load_finetuned_model(base_model_name, lora_path, use_gpu=True):\n","    \"\"\"Load the fine-tuned model for evaluation\"\"\"\n","\n","    print(\"Loading model for evaluation...\")\n","\n","    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"left\"  # Fix for decoder-only models\n","\n","    if use_gpu and torch.cuda.is_available():\n","        base_model = AutoModelForCausalLM.from_pretrained(\n","            base_model_name,\n","            torch_dtype=torch.float16,\n","            device_map=\"auto\",\n","            trust_remote_code=True\n","        )\n","    else:\n","        base_model = AutoModelForCausalLM.from_pretrained(\n","            base_model_name,\n","            torch_dtype=torch.float32,\n","            trust_remote_code=True\n","        )\n","\n","    # Load LoRA weights and merge\n","    model = PeftModel.from_pretrained(base_model, lora_path)\n","    model = model.merge_and_unload()\n","    model.eval()\n","\n","    print(\"Model loaded\\n\")\n","    return model, tokenizer"],"metadata":{"id":"doeJ5ZxNetRV","executionInfo":{"status":"ok","timestamp":1764048859620,"user_tz":300,"elapsed":81,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# LOAD EVALUATION DATASET\n","# ============================================================\n","def load_eval_dataset(dataset_path, max_samples=None):\n","    \"\"\"\n","    Load evaluation dataset\n","    Expected format: {\"prompt\": \"...\", \"reference\": \"...\"}\n","    or {\"text\": \"...\"}\n","    \"\"\"\n","\n","    print(f\"Loading evaluation dataset from {dataset_path}...\")\n","    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n","\n","    if max_samples:\n","        dataset = dataset.select(range(min(max_samples, len(dataset))))\n","\n","    print(f\"Loaded {len(dataset)} samples\\n\")\n","    return dataset\n","\n","def prepare_eval_data(dataset):\n","    \"\"\"\n","    Prepare evaluation data: extract prompts and references\n","    \"\"\"\n","    eval_data = []\n","\n","    for item in dataset:\n","        if \"prompt\" in item and \"reference\" in item:\n","            # Instruction format\n","            eval_data.append({\n","                \"prompt\": item[\"prompt\"],\n","                \"reference\": item[\"reference\"]\n","            })\n","        elif \"text\" in item:\n","            # Extract prompt and reference from full text\n","            # Assume format: prompt + reference code\n","            text = item[\"text\"]\n","            lines = text.split('\\n')\n","\n","            # Use first line/function signature as prompt\n","            prompt = lines[0] if lines else text[:100]\n","            reference = text\n","\n","            eval_data.append({\n","                \"prompt\": prompt,\n","                \"reference\": reference\n","            })\n","        elif \"instruction\" in item and \"output\" in item:\n","            eval_data.append({\n","                \"prompt\": item[\"instruction\"],\n","                \"reference\": item[\"output\"]\n","            })\n","\n","    return eval_data"],"metadata":{"id":"URCrbW1Iexqq","executionInfo":{"status":"ok","timestamp":1764048862010,"user_tz":300,"elapsed":48,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# GENERATE PREDICTIONS\n","# ============================================================\n","def generate_predictions(model, tokenizer, eval_data, max_length=256, batch_size=8):\n","    \"\"\"Generate predictions for all prompts\"\"\"\n","\n","    print(\"Generating predictions...\")\n","    predictions = []\n","\n","    use_gpu = torch.cuda.is_available()\n","\n","    # Process in batches\n","    for i in tqdm(range(0, len(eval_data), batch_size)):\n","        batch = eval_data[i:i+batch_size]\n","        prompts = [item[\"prompt\"] for item in batch]\n","\n","        # Tokenize\n","        inputs = tokenizer(\n","            prompts,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True,\n","            max_length=512\n","        )\n","\n","        if use_gpu:\n","            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n","\n","        # Generate\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=max_length,\n","                do_sample=False,  # Greedy for consistency\n","                pad_token_id=tokenizer.eos_token_id,\n","                use_cache=True,\n","            )\n","\n","        # Decode\n","        batch_predictions = [\n","            tokenizer.decode(output, skip_special_tokens=True)\n","            for output in outputs\n","        ]\n","        predictions.extend(batch_predictions)\n","\n","    print(\"Predictions generated\\n\")\n","    return predictions"],"metadata":{"id":"qwvGN2t9e1jf","executionInfo":{"status":"ok","timestamp":1764048865603,"user_tz":300,"elapsed":45,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# EVALUATION METRICS\n","# ============================================================\n","\n","def calculate_codebleu(predictions, references, language=\"python\"):\n","    \"\"\"\n","    Calculate CodeBLEU score\n","    CodeBLEU considers: BLEU, syntax match, dataflow match\n","    \"\"\"\n","    print(\"Calculating CodeBLEU...\")\n","\n","    # Map language names to codebleu format\n","    lang_mapping = {\n","        \"python\": \"python\",\n","        \"py\": \"python\",\n","        \"java\": \"java\",\n","        \"javascript\": \"javascript\",\n","        \"js\": \"javascript\",\n","        \"ts\": \"javascript\",\n","        \"cpp\": \"cpp\",\n","        \"c\": \"c\",\n","        \"c++\": \"cpp\",\n","        \"csharp\": \"c_sharp\",\n","        \"cs\": \"c_sharp\",\n","        \"go\": \"go\"\n","    }\n","\n","    codebleu_lang = lang_mapping.get(language.lower(), \"cpp\")\n","\n","    try:\n","        result = calc_codebleu(\n","            references=[[ref] for ref in references],\n","            predictions=predictions,\n","            lang=codebleu_lang,\n","            weights=(0.25, 0.25, 0.25, 0.25),\n","            tokenizer=None\n","        )\n","\n","        return {\n","            \"codebleu\": result[\"codebleu\"],\n","            \"ngram_match_score\": result[\"ngram_match_score\"],\n","            \"weighted_ngram_match_score\": result[\"weighted_ngram_match_score\"],\n","            \"syntax_match_score\": result[\"syntax_match_score\"],\n","            \"dataflow_match_score\": result[\"dataflow_match_score\"]\n","        }\n","    except Exception as e:\n","        print(f\"⚠ CodeBLEU calculation failed: {e}\")\n","        print(\"  Skipping CodeBLEU metric...\")\n","        return {\n","            \"codebleu\": None,\n","            \"ngram_match_score\": None,\n","            \"weighted_ngram_match_score\": None,\n","            \"syntax_match_score\": None,\n","            \"dataflow_match_score\": None\n","        }\n","\n","def calculate_bleu(predictions, references):\n","    \"\"\"Calculate BLEU score\"\"\"\n","    print(\"Calculating BLEU...\")\n","\n","    bleu = evaluate.load(\"bleu\")\n","\n","    # IMPORTANT: References must be list of lists\n","    # Each prediction maps to a list of reference(s)\n","    references_formatted = [[ref] for ref in references]\n","\n","    result = bleu.compute(\n","        predictions=predictions,\n","        references=references_formatted\n","    )\n","\n","    return result\n","\n","def calculate_exact_match(predictions, references):\n","    \"\"\"Calculate exact match accuracy\"\"\"\n","    print(\"Calculating Exact Match...\")\n","\n","    matches = sum(\n","        1 for pred, ref in zip(predictions, references)\n","        if pred.strip() == ref.strip()\n","    )\n","\n","    accuracy = matches / len(predictions) if predictions else 0\n","\n","    return {\n","        \"exact_match\": accuracy,\n","        \"total_matches\": matches,\n","        \"total_samples\": len(predictions)\n","    }\n","\n","def calculate_chrf(predictions, references):\n","    \"\"\"Calculate ChrF score (character n-gram F-score)\"\"\"\n","    print(\"Calculating ChrF...\")\n","\n","    chrf = evaluate.load(\"chrf\")\n","    result = chrf.compute(\n","        predictions=predictions,\n","        references=[[ref] for ref in references]\n","    )\n","\n","    return result\n","\n","def calculate_rouge(predictions, references):\n","    \"\"\"Calculate ROUGE scores\"\"\"\n","    print(\"Calculating ROUGE...\")\n","\n","    rouge = evaluate.load(\"rouge\")\n","    result = rouge.compute(\n","        predictions=predictions,\n","        references=references\n","    )\n","\n","    return result\n","\n","def calculate_edit_distance(predictions, references):\n","    \"\"\"Calculate average normalized edit distance\"\"\"\n","    print(\"Calculating Edit Distance...\")\n","\n","    from difflib import SequenceMatcher\n","\n","    distances = []\n","    for pred, ref in zip(predictions, references):\n","        ratio = SequenceMatcher(None, pred, ref).ratio()\n","        distances.append(ratio)\n","\n","    return {\n","        \"similarity_score\": np.mean(distances),\n","        \"avg_edit_distance\": 1 - np.mean(distances)\n","    }\n","\n","def calculate_syntax_validity(predictions, language=\"python\"):\n","    \"\"\"Check if generated code is syntactically valid\"\"\"\n","    print(\"Calculating Syntax Validity...\")\n","\n","    import ast\n","\n","    valid_count = 0\n","    syntax_errors = []\n","\n","    for i, pred in enumerate(predictions):\n","        try:\n","            if language == \"python\":\n","                ast.parse(pred)\n","                valid_count += 1\n","        except SyntaxError as e:\n","            syntax_errors.append((i, str(e)))\n","\n","    validity_rate = valid_count / len(predictions) if predictions else 0\n","\n","    return {\n","        \"syntax_validity\": validity_rate,\n","        \"valid_samples\": valid_count,\n","        \"total_samples\": len(predictions),\n","        \"sample_errors\": syntax_errors[:5]  # First 5 errors\n","    }\n"],"metadata":{"id":"KbLeGTkve67l","executionInfo":{"status":"ok","timestamp":1764048867974,"user_tz":300,"elapsed":85,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# COMPREHENSIVE EVALUATION\n","# ============================================================\n","def evaluate_model(\n","    model,\n","    tokenizer,\n","    eval_data,\n","    language=\"cpp\",\n","    max_length=256,\n","    batch_size=8\n","):\n","    \"\"\"\n","    Comprehensive model evaluation\n","    \"\"\"\n","\n","    print(\"=\"*60)\n","    print(\"STARTING COMPREHENSIVE EVALUATION\")\n","    print(\"=\"*60 + \"\\n\")\n","\n","    start_time = time.time()\n","\n","    # Generate predictions\n","    predictions = generate_predictions(\n","        model, tokenizer, eval_data, max_length, batch_size\n","    )\n","\n","    # Extract references\n","    references = [item[\"reference\"] for item in eval_data]\n","\n","    # Calculate all metrics\n","    results = {}\n","\n","    # 1. CodeBLEU (most important for code)\n","    codebleu_results = calculate_codebleu(predictions, references, language)\n","    results.update(codebleu_results)\n","\n","    # 2. BLEU\n","    bleu_results = calculate_bleu(predictions, references)\n","    results[\"bleu\"] = bleu_results[\"bleu\"]\n","    results[\"bleu_precisions\"] = bleu_results[\"precisions\"]\n","\n","    # 3. Exact Match\n","    exact_match_results = calculate_exact_match(predictions, references)\n","    results.update(exact_match_results)\n","\n","    # 4. ChrF\n","    chrf_results = calculate_chrf(predictions, references)\n","    results[\"chrf\"] = chrf_results[\"score\"]\n","\n","    # 5. ROUGE\n","    rouge_results = calculate_rouge(predictions, references)\n","    results[\"rouge1\"] = rouge_results[\"rouge1\"]\n","    results[\"rouge2\"] = rouge_results[\"rouge2\"]\n","    results[\"rougeL\"] = rouge_results[\"rougeL\"]\n","\n","    # 6. Edit Distance\n","    edit_results = calculate_edit_distance(predictions, references)\n","    results.update(edit_results)\n","\n","    # 7. Syntax Validity\n","    syntax_results = calculate_syntax_validity(predictions, language)\n","    results.update(syntax_results)\n","\n","    # Evaluation time\n","    eval_time = time.time() - start_time\n","    results[\"evaluation_time_seconds\"] = eval_time\n","    results[\"samples_per_second\"] = len(eval_data) / eval_time\n","\n","    return results, predictions, references"],"metadata":{"id":"umDrbbucfIU2","executionInfo":{"status":"ok","timestamp":1764048872607,"user_tz":300,"elapsed":37,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# PRINT RESULTS\n","# ============================================================\n","def print_results(results):\n","    \"\"\"Print evaluation results in a formatted way\"\"\"\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"EVALUATION RESULTS\")\n","    print(\"=\"*60 + \"\\n\")\n","\n","    # Helper to format metric\n","    def fmt(key, default=0.0):\n","        val = results.get(key, default)\n","        return f\"{val:.4f}\" if val is not None else \"N/A\"\n","\n","    print(\"CODE-SPECIFIC METRICS:\")\n","    print(\"-\" * 60)\n","    print(f\"  CodeBLEU:              {fmt('codebleu')}\")\n","    print(f\"    - N-gram Match:      {fmt('ngram_match_score')}\")\n","    print(f\"    - Weighted N-gram:   {fmt('weighted_ngram_match_score')}\")\n","    print(f\"    - Syntax Match:      {fmt('syntax_match_score')}\")\n","    print(f\"    - Dataflow Match:    {fmt('dataflow_match_score')}\")\n","    print(f\"  Syntax Validity:       {fmt('syntax_validity')}\")\n","\n","    print(\"\\nGENERAL METRICS:\")\n","    print(\"-\" * 60)\n","    print(f\"  BLEU:                  {fmt('bleu')}\")\n","    print(f\"  ChrF:                  {fmt('chrf')}\")\n","    print(f\"  ROUGE-1:               {fmt('rouge1')}\")\n","    print(f\"  ROUGE-2:               {fmt('rouge2')}\")\n","    print(f\"  ROUGE-L:               {fmt('rougeL')}\")\n","\n","    print(\"\\nACCURACY METRICS:\")\n","    print(\"-\" * 60)\n","    print(f\"  Exact Match:           {fmt('exact_match')}\")\n","    print(f\"  Similarity Score:      {fmt('similarity_score')}\")\n","    print(f\"  Edit Distance:         {fmt('avg_edit_distance')}\")\n","\n","    print(\"\\nPERFORMANCE:\")\n","    print(\"-\" * 60)\n","    print(f\"  Evaluation Time:       {results.get('evaluation_time_seconds', 0):.2f}s\")\n","    print(f\"  Samples/Second:        {results.get('samples_per_second', 0):.2f}\")\n","    print(f\"  Total Samples:         {results.get('total_samples', 0)}\")\n","\n","    print(\"\\n\" + \"=\"*60)\n","\n","def save_results(results, predictions, references, output_path=\"evaluation_results.json\"):\n","    \"\"\"Save evaluation results to file\"\"\"\n","\n","    output = {\n","        \"metrics\": results,\n","        \"samples\": [\n","            {\n","                \"prediction\": pred,\n","                \"reference\": ref\n","            }\n","            for pred, ref in zip(predictions[:10], references[:10])  # Save first 10 samples\n","        ]\n","    }\n","\n","    with open(output_path, \"w\") as f:\n","        json.dump(output, f, indent=2)\n","\n","    print(f\"\\n✓ Results saved to {output_path}\")"],"metadata":{"id":"YxkyLsATfQ-O","executionInfo":{"status":"ok","timestamp":1764048875473,"user_tz":300,"elapsed":44,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# MAIN EXECUTION\n","# ============================================================\n","# Configuration\n","BASE_MODEL = \"/content/drive/MyDrive/starcoder2-3b\"\n","LORA_PATH = \"/content/starcoder-finetuned\"\n","EVAL_DATASET_PATH = \"eval_dataset_fixed.json\"  # Your evaluation dataset\n","LANGUAGE = \"cpp\"  # or \"java\", \"javascript\", \"go\", etc.\n","MAX_SAMPLES = 100  # Set to None to evaluate all samples\n","\n","print(\"=\"*60)\n","print(\"MODEL EVALUATION PIPELINE\")\n","print(\"=\"*60 + \"\\n\")\n","\n","# Load model\n","model, tokenizer = load_finetuned_model(BASE_MODEL, LORA_PATH)\n","\n","# Load evaluation dataset\n","eval_dataset = load_eval_dataset(EVAL_DATASET_PATH, max_samples=MAX_SAMPLES)\n","eval_data = prepare_eval_data(eval_dataset)\n","\n","print(f\"Evaluating on {len(eval_data)} samples...\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257,"referenced_widgets":["638e80e4667b433088a39fdb1356d19b","5ca8bce06d9c4fe9a591623fb803b984","e3de5fe9693144faa33fd8262ff1749e","f0312fc63016475b86cf933bb870b50d","6b393112d49c4d49a2d727518c47d3fd","3cbde2259fd14a9db24ff604ed5e077f","06e76a545ffc4883be4811b738b7cc25","493aa763500b4810955a0f8716c35843","892325382abe46d2a9120bf21cb5c21d","02460803174f4e4b844fd9eef60daa49","4bfdab0c7d8f47149eff6f4748aae7e6"]},"id":"AgnFZ0Q5ajTd","executionInfo":{"status":"ok","timestamp":1764049826255,"user_tz":300,"elapsed":68555,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}},"outputId":"f577007d-d383-46e6-9a39-99bec2ba2dd0"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","MODEL EVALUATION PIPELINE\n","============================================================\n","\n","Loading model for evaluation...\n","Model loaded\n","\n","Loading evaluation dataset from eval_dataset_fixed.json...\n"]},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"638e80e4667b433088a39fdb1356d19b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded 42 samples\n","\n","Evaluating on 42 samples...\n","\n"]}]},{"cell_type":"code","source":["# Run evaluation\n","results, predictions, references = evaluate_model(\n","    model=model,\n","    tokenizer=tokenizer,\n","    eval_data=eval_data,\n","    language=LANGUAGE,\n","    max_length=256,\n","    batch_size=8\n",")\n","\n","# Print results\n","print_results(results)\n","\n","# Save results\n","save_results(results, predictions, references)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KVecGbYUfhoU","executionInfo":{"status":"ok","timestamp":1764049898206,"user_tz":300,"elapsed":63666,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}},"outputId":"9db7e98c-5074-4fbd-85af-30adad82af4f"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","STARTING COMPREHENSIVE EVALUATION\n","============================================================\n","\n","Generating predictions...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6/6 [00:54<00:00,  9.02s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Predictions generated\n","\n","Calculating CodeBLEU...\n","⚠ CodeBLEU calculation failed: an integer is required\n","  Skipping CodeBLEU metric...\n","Calculating BLEU...\n","Calculating Exact Match...\n","Calculating ChrF...\n","Calculating ROUGE...\n","Calculating Edit Distance...\n","Calculating Syntax Validity...\n","\n","============================================================\n","EVALUATION RESULTS\n","============================================================\n","\n","CODE-SPECIFIC METRICS:\n","------------------------------------------------------------\n","  CodeBLEU:              N/A\n","    - N-gram Match:      N/A\n","    - Weighted N-gram:   N/A\n","    - Syntax Match:      N/A\n","    - Dataflow Match:    N/A\n","  Syntax Validity:       0.0000\n","\n","GENERAL METRICS:\n","------------------------------------------------------------\n","  BLEU:                  0.0000\n","  ChrF:                  3.3889\n","  ROUGE-1:               0.1652\n","  ROUGE-2:               0.0966\n","  ROUGE-L:               0.1353\n","\n","ACCURACY METRICS:\n","------------------------------------------------------------\n","  Exact Match:           0.0000\n","  Similarity Score:      0.0961\n","  Edit Distance:         0.9039\n","\n","PERFORMANCE:\n","------------------------------------------------------------\n","  Evaluation Time:       63.66s\n","  Samples/Second:        0.66\n","  Total Samples:         42\n","\n","============================================================\n","\n","✓ Results saved to evaluation_results.json\n"]}]},{"cell_type":"code","source":["# Print sample comparisons\n","print(\"\\n\" + \"=\"*60)\n","print(\"SAMPLE PREDICTIONS (first 3)\")\n","print(\"=\"*60)\n","\n","for i in range(min(3, len(predictions))):\n","    print(f\"\\n--- Sample {i+1} ---\")\n","    print(f\"Prompt:\\n{eval_data[i]['prompt']}\")\n","    print(f\"\\nReference:\\n{references[i][:200]}...\")\n","    print(f\"\\nPrediction:\\n{predictions[i][:200]}...\")\n","    print(\"-\" * 60)\n","\n","print(\"\\n✓ Evaluation complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Nb6NQX-fkuD","executionInfo":{"status":"ok","timestamp":1764049906903,"user_tz":300,"elapsed":26,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}},"outputId":"b2d10249-53ed-4215-cb90-f9e8bf4969e2"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","SAMPLE PREDICTIONS (first 3)\n","============================================================\n","\n","--- Sample 1 ---\n","Prompt:\n","UNIT_TEST(Assert_Smoke)\n","\n","Reference:\n","#include \"testing/testing.hpp\"\n","\n","#include \"base/base.hpp\"\n","#include \"base/exception.hpp\"\n","#include \"base/logging.hpp\"\n","\n","\n","UNIT_TEST(Assert_Smoke)\n","{\n","  int x = 5;\n","  // to avoid warning in release\n","#ifdef RELE...\n","\n","Prediction:\n","UNIT_TEST(Assert_Smoke)\n","{\n","    // Arrange\n","    auto const expected = std::vector<std::string>{ \"a\", \"b\", \"c\" };\n","    auto const actual = std::vector<std::string>{ \"a\", \"b\", \"c\" };\n","\n","    // Act\n","    Assert:...\n","------------------------------------------------------------\n","\n","--- Sample 2 ---\n","Prompt:\n","void SectionParser::parse(std::istream& input, ForceField& ff, int lineNo)\n","\n","Reference:\n","/*\n"," * Copyright (c) 2004-present, The University of Notre Dame. All rights\n"," * reserved.\n"," *\n"," * Redistribution and use in source and binary forms, with or without\n"," * modification, are permitted provided...\n","\n","Prediction:\n","void SectionParser::parse(std::istream& input, ForceField& ff, int lineNo)\n","{\n","    std::string line;\n","    std::getline(input, line);\n","    lineNo++;\n","    if (line.empty()) {\n","        return;\n","    }\n","    if (li...\n","------------------------------------------------------------\n","\n","--- Sample 3 ---\n","Prompt:\n","auto today = budget::local_day();\n","\n","Reference:\n","//===\n","// Copyright (c) 2013-2020 Baptiste Wicht.\n","// Distributed under the terms of the MIT License.\n","// (See accompanying file LICENSE or copy at\n","//  https://example.com/path\n","//===\n","\n","#include <utility>\n","...\n","\n","Prediction:\n","auto today = budget::local_day();\n","\tauto today_str = today.to_string();\n","\tauto today_str_no_time = today_str.substr(0, today_str.find(' '));\n","\tauto today_str_no_time_no_dash = today_str_no_time.substr(0,...\n","------------------------------------------------------------\n","\n","✓ Evaluation complete!\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# Diagnose Evaluation Dataset Issues\n","# ============================================================\n","\n","import json\n","from collections import Counter\n","\n","def diagnose_eval_dataset(file_path=\"eval_dataset.json\"):\n","    \"\"\"\n","    Analyze evaluation dataset to identify issues\n","    \"\"\"\n","\n","    print(\"=\"*60)\n","    print(\"EVALUATION DATASET DIAGNOSTICS\")\n","    print(\"=\"*60 + \"\\n\")\n","\n","    # Load dataset\n","    with open(file_path, \"r\") as f:\n","        data = json.load(f)\n","\n","    print(f\"Total samples: {len(data)}\\n\")\n","\n","    # Check structure\n","    print(\"Dataset Structure:\")\n","    print(\"-\" * 60)\n","    if data:\n","        print(f\"Keys in first sample: {list(data[0].keys())}\")\n","        print()\n","\n","    # Check languages\n","    languages = Counter()\n","    for item in data:\n","        lang = item.get(\"language\", \"unknown\")\n","        languages[lang] += 1\n","\n","    print(\"Language Distribution:\")\n","    print(\"-\" * 60)\n","    for lang, count in languages.most_common():\n","        print(f\"  {lang}: {count} samples ({count/len(data)*100:.1f}%)\")\n","    print()\n","\n","    # Check for mixed content\n","    has_prompt = sum(1 for item in data if \"prompt\" in item)\n","    has_reference = sum(1 for item in data if \"reference\" in item)\n","    has_text = sum(1 for item in data if \"text\" in item)\n","\n","    print(\"Field Coverage:\")\n","    print(\"-\" * 60)\n","    print(f\"  Has 'prompt': {has_prompt}/{len(data)} ({has_prompt/len(data)*100:.1f}%)\")\n","    print(f\"  Has 'reference': {has_reference}/{len(data)} ({has_reference/len(data)*100:.1f}%)\")\n","    print(f\"  Has 'text': {has_text}/{len(data)} ({has_text/len(data)*100:.1f}%)\")\n","    print()\n","\n","    # Check for empty or very short samples\n","    empty_prompts = sum(1 for item in data if not item.get(\"prompt\", \"\").strip())\n","    empty_refs = sum(1 for item in data if not item.get(\"reference\", \"\").strip())\n","    short_refs = sum(1 for item in data if len(item.get(\"reference\", \"\")) < 10)\n","\n","    print(\"Data Quality:\")\n","    print(\"-\" * 60)\n","    print(f\"  Empty prompts: {empty_prompts}\")\n","    print(f\"  Empty references: {empty_refs}\")\n","    print(f\"  Very short references (<10 chars): {short_refs}\")\n","    print()\n","\n","    # Sample prompts and references\n","    print(\"Sample Entries:\")\n","    print(\"-\" * 60)\n","    for i in range(min(3, len(data))):\n","        item = data[i]\n","        print(f\"\\nSample {i+1}:\")\n","        print(f\"  Language: {item.get('language', 'N/A')}\")\n","        print(f\"  Prompt: {item.get('prompt', 'N/A')[:80]}...\")\n","        print(f\"  Reference length: {len(item.get('reference', ''))} chars\")\n","        ref_preview = item.get('reference', '')[:100].replace('\\n', ' ')\n","        print(f\"  Reference preview: {ref_preview}...\")\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"RECOMMENDATIONS:\")\n","    print(\"=\"*60)\n","\n","    # Recommendations\n","    if len(languages) > 3:\n","        print(\"⚠ You have many languages mixed. Consider:\")\n","        print(\"  1. Evaluate each language separately\")\n","        print(\"  2. Use the primary language for CodeBLEU\")\n","\n","    if empty_prompts > 0 or empty_refs > 0:\n","        print(\"⚠ Some samples have empty prompts/references\")\n","        print(\"  Run: clean_eval_dataset()\")\n","\n","    if short_refs > len(data) * 0.3:\n","        print(\"⚠ Many references are very short\")\n","        print(\"  This may affect metric reliability\")\n","\n","    print()\n","\n","    return data\n","\n","def clean_eval_dataset(input_file=\"eval_dataset.json\", output_file=\"eval_dataset_clean.json\"):\n","    \"\"\"\n","    Clean evaluation dataset by removing problematic samples\n","    \"\"\"\n","\n","    print(\"Cleaning evaluation dataset...\")\n","\n","    with open(input_file, \"r\") as f:\n","        data = json.load(f)\n","\n","    original_count = len(data)\n","\n","    # Filter out problematic samples\n","    clean_data = []\n","    for item in data:\n","        prompt = item.get(\"prompt\", \"\").strip()\n","        reference = item.get(\"reference\", \"\").strip()\n","\n","        # Keep if both prompt and reference are non-empty\n","        if prompt and reference and len(reference) >= 10:\n","            clean_data.append(item)\n","\n","    # Save cleaned data\n","    with open(output_file, \"w\") as f:\n","        json.dump(clean_data, f, indent=2)\n","\n","    removed = original_count - len(clean_data)\n","    print(f\"✓ Cleaned dataset saved to {output_file}\")\n","    print(f\"  Original: {original_count} samples\")\n","    print(f\"  Cleaned: {len(clean_data)} samples\")\n","    print(f\"  Removed: {removed} samples\")\n","\n","    return clean_data\n","\n","def split_by_language(input_file=\"eval_dataset.json\"):\n","    \"\"\"\n","    Split evaluation dataset by language for separate evaluation\n","    \"\"\"\n","\n","    print(\"Splitting dataset by language...\")\n","\n","    with open(input_file, \"r\") as f:\n","        data = json.load(f)\n","\n","    # Group by language\n","    by_language = {}\n","    for item in data:\n","        lang = item.get(\"language\", \"unknown\")\n","        if lang not in by_language:\n","            by_language[lang] = []\n","        by_language[lang].append(item)\n","\n","    # Save each language separately\n","    for lang, items in by_language.items():\n","        output_file = f\"eval_dataset_{lang}.json\"\n","        with open(output_file, \"w\") as f:\n","            json.dump(items, f, indent=2)\n","        print(f\"✓ {lang}: {len(items)} samples → {output_file}\")\n","\n","    print(f\"\\nTotal languages: {len(by_language)}\")\n","\n","    return by_language\n","\n","# ============================================================\n","# RUN DIAGNOSTICS\n","# ============================================================\n","if __name__ == \"__main__\":\n","\n","    # Diagnose the dataset\n","    data = diagnose_eval_dataset(\"eval_dataset.json\")\n","\n","    # Optional: Clean the dataset\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Would you like to clean the dataset? (Uncomment below)\")\n","    print(\"=\"*60)\n","    # clean_data = clean_eval_dataset(\"eval_dataset.json\", \"eval_dataset_clean.json\")\n","\n","    # Optional: Split by language\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"Would you like to split by language? (Uncomment below)\")\n","    print(\"=\"*60)\n","    # split_by_language(\"eval_dataset.json\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUFqdF-RjQ-M","executionInfo":{"status":"ok","timestamp":1764049624589,"user_tz":300,"elapsed":53,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}},"outputId":"2a74aaaa-172e-441d-a36f-e108bf2317aa"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","EVALUATION DATASET DIAGNOSTICS\n","============================================================\n","\n","Total samples: 42\n","\n","Dataset Structure:\n","------------------------------------------------------------\n","Keys in first sample: ['prompt', 'reference', 'language', 'file_name']\n","\n","Language Distribution:\n","------------------------------------------------------------\n","  cpp: 42 samples (100.0%)\n","\n","Field Coverage:\n","------------------------------------------------------------\n","  Has 'prompt': 42/42 (100.0%)\n","  Has 'reference': 42/42 (100.0%)\n","  Has 'text': 0/42 (0.0%)\n","\n","Data Quality:\n","------------------------------------------------------------\n","  Empty prompts: 0\n","  Empty references: 0\n","  Very short references (<10 chars): 0\n","\n","Sample Entries:\n","------------------------------------------------------------\n","\n","Sample 1:\n","  Language: cpp\n","  Prompt: UNIT_TEST(Assert_Smoke)...\n","  Reference length: 722 chars\n","  Reference preview: #include \"testing/testing.hpp\"  #include \"base/base.hpp\" #include \"base/exception.hpp\" #include \"bas...\n","\n","Sample 2:\n","  Language: cpp\n","  Prompt: * Copyright (c) 2004-present, The University of Notre Dame. All rights...\n","  Reference length: 4779 chars\n","  Reference preview: /*  * Copyright (c) 2004-present, The University of Notre Dame. All rights  * reserved.  *  * Redist...\n","\n","Sample 3:\n","  Language: cpp\n","  Prompt: // Copyright (c) 2013-2020 Baptiste Wicht....\n","  Reference length: 4160 chars\n","  Reference preview: //=== // Copyright (c) 2013-2020 Baptiste Wicht. // Distributed under the terms of the MIT License. ...\n","\n","============================================================\n","RECOMMENDATIONS:\n","============================================================\n","\n","\n","============================================================\n","Would you like to clean the dataset? (Uncomment below)\n","============================================================\n","\n","============================================================\n","Would you like to split by language? (Uncomment below)\n","============================================================\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# Fix C++ Evaluation Dataset - Extract Proper Code Prompts\n","# ============================================================\n","\n","import json\n","import re\n","\n","def extract_cpp_prompt(code):\n","    \"\"\"\n","    Extract a meaningful C++ prompt from code\n","\n","    Strategy:\n","    1. Skip comments and copyright headers\n","    2. Find function signatures\n","    3. Find class declarations\n","    4. Use first meaningful code line\n","    \"\"\"\n","\n","    lines = code.split('\\n')\n","\n","    # Skip initial comments and includes\n","    code_start = 0\n","    for i, line in enumerate(lines):\n","        stripped = line.strip()\n","\n","        # Skip empty lines, comments, includes, copyright\n","        if (not stripped or\n","            stripped.startswith('//') or\n","            stripped.startswith('/*') or\n","            stripped.startswith('*') or\n","            stripped.startswith('#include') or\n","            stripped.startswith('#define') or\n","            stripped.startswith('#pragma')):\n","            continue\n","\n","        code_start = i\n","        break\n","\n","    # Get code without headers\n","    code_lines = lines[code_start:]\n","\n","    if not code_lines:\n","        # Fallback: use first line\n","        return lines[0] if lines else \"\"\n","\n","    # Strategy 1: Find function signature\n","    for i, line in enumerate(code_lines):\n","        stripped = line.strip()\n","\n","        # Function declaration/definition patterns\n","        if (('(' in stripped and ')' in stripped and\n","             not stripped.startswith('//') and\n","             not stripped.startswith('/*')) and\n","            any(keyword in stripped for keyword in ['void', 'int', 'bool', 'double',\n","                                                     'float', 'char', 'auto', 'string',\n","                                                     'TEST', 'UNIT_TEST'])):\n","\n","            # Get function signature (might span multiple lines)\n","            signature = stripped\n","\n","            # If line doesn't end with { or ;, it might continue\n","            if not (signature.endswith('{') or signature.endswith(';')):\n","                # Look for continuation\n","                for j in range(i+1, min(i+3, len(code_lines))):\n","                    signature += ' ' + code_lines[j].strip()\n","                    if '{' in code_lines[j] or ';' in code_lines[j]:\n","                        break\n","\n","            # Clean up the signature - remove { at end\n","            signature = signature.replace('{', '').strip()\n","\n","            return signature\n","\n","    # Strategy 2: Find class declaration\n","    for line in code_lines:\n","        stripped = line.strip()\n","        if stripped.startswith('class ') or stripped.startswith('struct '):\n","            return stripped.replace('{', '').strip()\n","\n","    # Strategy 3: Find template or namespace\n","    for line in code_lines:\n","        stripped = line.strip()\n","        if stripped.startswith('template') or stripped.startswith('namespace'):\n","            return stripped\n","\n","    # Fallback: use first non-empty line\n","    for line in code_lines:\n","        stripped = line.strip()\n","        if stripped:\n","            return stripped\n","\n","    return code_lines[0] if code_lines else \"\"\n","\n","def fix_eval_dataset(\n","    input_file=\"eval_dataset.json\",\n","    output_file=\"eval_dataset_fixed.json\"\n","):\n","    \"\"\"\n","    Fix evaluation dataset by extracting proper C++ prompts\n","    \"\"\"\n","\n","    print(\"=\"*60)\n","    print(\"FIXING C++ EVALUATION DATASET\")\n","    print(\"=\"*60 + \"\\n\")\n","\n","    # Load dataset\n","    with open(input_file, \"r\") as f:\n","        data = json.load(f)\n","\n","    print(f\"Original dataset: {len(data)} samples\\n\")\n","\n","    # Fix each sample\n","    fixed_data = []\n","    skipped = 0\n","\n","    for i, item in enumerate(data):\n","        reference = item[\"reference\"]\n","\n","        # Extract better prompt\n","        new_prompt = extract_cpp_prompt(reference)\n","\n","        # Skip if prompt is still a comment or too short\n","        if (new_prompt.startswith('//') or\n","            new_prompt.startswith('/*') or\n","            new_prompt.startswith('*') or\n","            len(new_prompt) < 10):\n","            print(f\"⚠ Skipping sample {i+1}: Invalid prompt extracted\")\n","            skipped += 1\n","            continue\n","\n","        # Create fixed item\n","        fixed_item = {\n","            \"prompt\": new_prompt,\n","            \"reference\": reference,\n","            \"language\": item[\"language\"],\n","            \"file_name\": item[\"file_name\"],\n","            \"original_prompt\": item[\"prompt\"]  # Keep original for reference\n","        }\n","\n","        fixed_data.append(fixed_item)\n","\n","        # Show progress for first few\n","        if i < 3:\n","            print(f\"Sample {i+1}:\")\n","            print(f\"  Old prompt: {item['prompt'][:60]}...\")\n","            print(f\"  New prompt: {new_prompt[:60]}...\")\n","            print()\n","\n","    # Save fixed dataset\n","    with open(output_file, \"w\") as f:\n","        json.dump(fixed_data, f, indent=2)\n","\n","    print(\"=\"*60)\n","    print(\"RESULTS\")\n","    print(\"=\"*60)\n","    print(f\"Original samples: {len(data)}\")\n","    print(f\"Fixed samples: {len(fixed_data)}\")\n","    print(f\"Skipped: {skipped}\")\n","    print(f\"\\n✓ Fixed dataset saved to {output_file}\")\n","    print(\"=\"*60 + \"\\n\")\n","\n","    # Show some examples\n","    print(\"Sample Fixed Entries:\")\n","    print(\"-\" * 60)\n","    for i in range(min(3, len(fixed_data))):\n","        item = fixed_data[i]\n","        print(f\"\\nSample {i+1}:\")\n","        print(f\"  Prompt: {item['prompt'][:100]}\")\n","        print(f\"  Reference length: {len(item['reference'])} chars\")\n","\n","    return fixed_data\n","\n","def create_simplified_eval_dataset(\n","    input_file=\"eval_dataset.json\",\n","    output_file=\"eval_dataset_simple.json\",\n","    max_samples=50,\n","    max_reference_length=500\n","):\n","    \"\"\"\n","    Create simplified evaluation dataset\n","    - Better prompts\n","    - Shorter references (for faster evaluation)\n","    - Filter for quality\n","    \"\"\"\n","\n","    print(\"=\"*60)\n","    print(\"CREATING SIMPLIFIED EVALUATION DATASET\")\n","    print(\"=\"*60 + \"\\n\")\n","\n","    with open(input_file, \"r\") as f:\n","        data = json.load(f)\n","\n","    simplified_data = []\n","\n","    for item in data:\n","        reference = item[\"reference\"]\n","\n","        # Extract prompt\n","        prompt = extract_cpp_prompt(reference)\n","\n","        # Skip bad prompts\n","        if (prompt.startswith('//') or prompt.startswith('/*') or\n","            prompt.startswith('*') or len(prompt) < 10):\n","            continue\n","\n","        # Truncate long references (keep first function/class only)\n","        if len(reference) > max_reference_length:\n","            # Try to find end of first function/class\n","            lines = reference.split('\\n')\n","            truncated_lines = []\n","            brace_count = 0\n","            started = False\n","\n","            for line in lines:\n","                truncated_lines.append(line)\n","\n","                # Count braces\n","                if '{' in line:\n","                    brace_count += line.count('{')\n","                    started = True\n","                if '}' in line:\n","                    brace_count -= line.count('}')\n","\n","                # Stop when we've closed all braces\n","                if started and brace_count == 0:\n","                    break\n","\n","                # Safety: don't go too long\n","                if len('\\n'.join(truncated_lines)) > max_reference_length:\n","                    break\n","\n","            reference = '\\n'.join(truncated_lines)\n","\n","        simplified_data.append({\n","            \"prompt\": prompt,\n","            \"reference\": reference,\n","            \"language\": \"cpp\"\n","        })\n","\n","        if len(simplified_data) >= max_samples:\n","            break\n","\n","    # Save\n","    with open(output_file, \"w\") as f:\n","        json.dump(simplified_data, f, indent=2)\n","\n","    print(f\"✓ Created {len(simplified_data)} simplified samples\")\n","    print(f\"  Saved to {output_file}\")\n","    print(f\"  Average reference length: {sum(len(x['reference']) for x in simplified_data) / len(simplified_data):.0f} chars\")\n","\n","    return simplified_data\n","\n","# ============================================================\n","# MAIN EXECUTION\n","# ============================================================\n","if __name__ == \"__main__\":\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"OPTION 1: Fix existing dataset (better prompts)\")\n","    print(\"=\"*60)\n","    fixed_data = fix_eval_dataset(\n","        \"eval_dataset.json\",\n","        \"eval_dataset_fixed.json\"\n","    )\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"OPTION 2: Create simplified dataset (shorter, faster)\")\n","    print(\"=\"*60)\n","    simple_data = create_simplified_eval_dataset(\n","        \"eval_dataset.json\",\n","        \"eval_dataset_simple.json\",\n","        max_samples=30,\n","        max_reference_length=300\n","    )\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"NEXT STEPS\")\n","    print(\"=\"*60)\n","    print(\"\"\"\n","1. Use fixed dataset for comprehensive evaluation:\n","   python evaluate_model.py  # Update: EVAL_DATASET_PATH = \"eval_dataset_fixed.json\"\n","\n","2. Or use simplified dataset for quick evaluation:\n","   python evaluate_model.py  # Update: EVAL_DATASET_PATH = \"eval_dataset_simple.json\"\n","\n","3. The simplified dataset will be faster (shorter code = faster generation)\n","    \"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cQxdekKAjnqC","executionInfo":{"status":"ok","timestamp":1764049717616,"user_tz":300,"elapsed":71,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}},"outputId":"eeb2e062-fdc6-4d88-e9d8-1007def91f3f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","OPTION 1: Fix existing dataset (better prompts)\n","============================================================\n","============================================================\n","FIXING C++ EVALUATION DATASET\n","============================================================\n","\n","Original dataset: 42 samples\n","\n","Sample 1:\n","  Old prompt: UNIT_TEST(Assert_Smoke)...\n","  New prompt: UNIT_TEST(Assert_Smoke)...\n","\n","Sample 2:\n","  Old prompt: * Copyright (c) 2004-present, The University of Notre Dame. ...\n","  New prompt: void SectionParser::parse(std::istream& input, ForceField& f...\n","\n","Sample 3:\n","  Old prompt: // Copyright (c) 2013-2020 Baptiste Wicht....\n","  New prompt: auto today = budget::local_day();...\n","\n","============================================================\n","RESULTS\n","============================================================\n","Original samples: 42\n","Fixed samples: 42\n","Skipped: 0\n","\n","✓ Fixed dataset saved to eval_dataset_fixed.json\n","============================================================\n","\n","Sample Fixed Entries:\n","------------------------------------------------------------\n","\n","Sample 1:\n","  Prompt: UNIT_TEST(Assert_Smoke)\n","  Reference length: 722 chars\n","\n","Sample 2:\n","  Prompt: void SectionParser::parse(std::istream& input, ForceField& ff, int lineNo)\n","  Reference length: 4779 chars\n","\n","Sample 3:\n","  Prompt: auto today = budget::local_day();\n","  Reference length: 4160 chars\n","\n","============================================================\n","OPTION 2: Create simplified dataset (shorter, faster)\n","============================================================\n","============================================================\n","CREATING SIMPLIFIED EVALUATION DATASET\n","============================================================\n","\n","✓ Created 30 simplified samples\n","  Saved to eval_dataset_simple.json\n","  Average reference length: 303 chars\n","\n","============================================================\n","NEXT STEPS\n","============================================================\n","\n","1. Use fixed dataset for comprehensive evaluation:\n","   python evaluate_model.py  # Update: EVAL_DATASET_PATH = \"eval_dataset_fixed.json\"\n","\n","2. Or use simplified dataset for quick evaluation:\n","   python evaluate_model.py  # Update: EVAL_DATASET_PATH = \"eval_dataset_simple.json\"\n","\n","3. The simplified dataset will be faster (shorter code = faster generation)\n","    \n"]}]}]}