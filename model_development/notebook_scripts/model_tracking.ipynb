{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aJWQUp82Nzn0jttZJ8o8CnvnoBSrG1-q","timestamp":1764046510972}],"gpuType":"T4","mount_file_id":"1aJWQUp82Nzn0jttZJ8o8CnvnoBSrG1-q","authorship_tag":"ABX9TyNY5FIC0mcnbLCCZIJH1bZ4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7adb7df0d286496ab6af41f0661b9d65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a21516eb4354f848e2b2f8636ec302c","IPY_MODEL_1f239c2bf46d45ddae7c465334b886b9","IPY_MODEL_6cd995d9ca4c4a2d969bdd35c4bafe19"],"layout":"IPY_MODEL_56c2d0100a4748d1aae1a7c2722d8d52"}},"0a21516eb4354f848e2b2f8636ec302c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de6c98fd193f452595333b7c6993af95","placeholder":"​","style":"IPY_MODEL_c6d45285a0cd49e79f841390d1a42820","value":"Generating train split: "}},"1f239c2bf46d45ddae7c465334b886b9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b8ef255f6a64042a449c4363603f6e7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7546600a695470db38d202427d2dc58","value":1}},"6cd995d9ca4c4a2d969bdd35c4bafe19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_253b542ca8e543509ed9a792e32ebba5","placeholder":"​","style":"IPY_MODEL_3454c1e3a26d433493f5ffd7452c65e3","value":" 42/0 [00:00&lt;00:00, 50.86 examples/s]"}},"56c2d0100a4748d1aae1a7c2722d8d52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de6c98fd193f452595333b7c6993af95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6d45285a0cd49e79f841390d1a42820":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b8ef255f6a64042a449c4363603f6e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"f7546600a695470db38d202427d2dc58":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"253b542ca8e543509ed9a792e32ebba5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3454c1e3a26d433493f5ffd7452c65e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee6ea39fde3547c9a32fdd702368f662":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4956e2273b144239c2d1d37d67f7873","IPY_MODEL_5ad4c17d7f6f4ccd950a8b639141c4c8","IPY_MODEL_56377646990340dc910250efb14e1fe9"],"layout":"IPY_MODEL_5e70bdb47fc5456d8b4f46df11bad954"}},"e4956e2273b144239c2d1d37d67f7873":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5aed33ec33b4f56be7ebf955ef1ce6b","placeholder":"​","style":"IPY_MODEL_f1fcebb450a6497e8c394315d9d30ace","value":"Map: 100%"}},"5ad4c17d7f6f4ccd950a8b639141c4c8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5db7f09f6d14ed48d691abd0ba8e096","max":42,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d2c5f2d0be834a168b5975dc1a234a3e","value":42}},"56377646990340dc910250efb14e1fe9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ba187c8f304428d9cdf728e5977be4a","placeholder":"​","style":"IPY_MODEL_c43f16b790a84b7da9d4935f33ba08e1","value":" 42/42 [00:00&lt;00:00, 97.74 examples/s]"}},"5e70bdb47fc5456d8b4f46df11bad954":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5aed33ec33b4f56be7ebf955ef1ce6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1fcebb450a6497e8c394315d9d30ace":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5db7f09f6d14ed48d691abd0ba8e096":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2c5f2d0be834a168b5975dc1a234a3e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7ba187c8f304428d9cdf728e5977be4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c43f16b790a84b7da9d4935f33ba08e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# ============================================================\n","# Complete StarCoder2-3B Training with MLflow + Retraining\n","# Copy this ENTIRE script into ONE cell in Colab\n","# ============================================================\n","\n","# Clean start\n","import os, shutil\n","for d in [\"./mlruns\", \"./mlartifacts\"]:\n","    if os.path.exists(d):\n","        shutil.rmtree(d)\n","print(\"✓ Cleaned directories\\n\")\n","\n","# Install packages\n","import subprocess, sys\n","packages = [\"transformers\", \"datasets\", \"peft\", \"accelerate\", \"bitsandbytes\", \"mlflow\"]\n","print(\"Installing packages...\")\n","for pkg in packages:\n","    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg],\n","                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n","print(\"✓ Packages installed\\n\")\n","\n","# Import everything\n","import torch\n","import mlflow\n","from datetime import datetime\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer,\n","    DataCollatorForLanguageModeling, BitsAndBytesConfig, TrainerCallback\n",")\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n","\n","print(\"✓ Imports complete\\n\")\n","\n","# Setup MLflow\n","os.makedirs(\"./mlruns\", exist_ok=True)\n","mlflow.set_tracking_uri(\"file://./mlruns\")\n","exp_name = f\"starcoder-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n","mlflow.create_experiment(exp_name)\n","mlflow.set_experiment(exp_name)\n","print(f\"✓ MLflow experiment: {exp_name}\\n\")\n","\n","# Config\n","CONFIG = {\n","    \"model_name\": \"/content/drive/MyDrive//starcoder2-3b\",\n","    \"output_dir\": \"./starcoder-finetuned\",\n","    \"dataset_path\": \"/content/drive/MyDrive/code_dataset.json\",\n","    \"lora_r\": 16, \"lora_alpha\": 32, \"lora_dropout\": 0.05,\n","    \"batch_size\": 4, \"gradient_accumulation_steps\": 4,\n","    \"num_epochs\": 3, \"learning_rate\": 2e-4,\n","    \"max_length\": 512, \"warmup_steps\": 50,\n","}\n","\n","# GPU check\n","USE_GPU = torch.cuda.is_available()\n","if USE_GPU:\n","    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\\n\")\n","else:\n","    print(\"⚠ Using CPU (slow)\\n\")\n","\n","# MLflow callback\n","class MLflowCallback(TrainerCallback):\n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        if logs:\n","            for k, v in logs.items():\n","                if isinstance(v, (int, float)):\n","                    try:\n","                        mlflow.log_metric(k, v, step=state.global_step)\n","                    except:\n","                        pass\n","\n","# Load model function\n","def load_model(name):\n","    print(f\"Loading {name}...\")\n","    tokenizer = AutoTokenizer.from_pretrained(name, trust_remote_code=True)\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    if USE_GPU:\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True\n","        )\n","        model = AutoModelForCausalLM.from_pretrained(\n","            name, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True\n","        )\n","        model = prepare_model_for_kbit_training(model)\n","    else:\n","        model = AutoModelForCausalLM.from_pretrained(\n","            name, torch_dtype=torch.float32, low_cpu_mem_usage=True, trust_remote_code=True\n","        )\n","\n","    model.config.use_cache = False\n","    print(\"✓ Model loaded\\n\")\n","    return model, tokenizer\n","\n","# Apply LoRA\n","def apply_lora(model, cfg):\n","    # Freeze all base model parameters first\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    lora_cfg = LoraConfig(\n","        r=cfg[\"lora_r\"], lora_alpha=cfg[\"lora_alpha\"],\n","        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n","        lora_dropout=cfg[\"lora_dropout\"], bias=\"none\", task_type=\"CAUSAL_LM\"\n","    )\n","    model = get_peft_model(model, lora_cfg)\n","\n","    # Enable gradient checkpointing and input gradients\n","    model.enable_input_require_grads()\n","\n","    print()\n","    model.print_trainable_parameters()\n","    print()\n","    return model\n","\n","# Load dataset\n","def load_data(path, tokenizer, max_len):\n","    print(f\"Loading {path}...\")\n","    ds = load_dataset(\"json\", data_files=path, split=\"train\")\n","    print(f\"✓ {len(ds)} examples\\n\")\n","\n","    def preprocess(examples):\n","        texts = examples[\"text\"] if \"text\" in examples else [\n","            f\"### Instruction:\\n{i}\\n\\n### Response:\\n{o}\"\n","            for i, o in zip(examples[\"instruction\"], examples[\"output\"])\n","        ]\n","        result = tokenizer(texts, truncation=True, max_length=max_len,\n","                          padding=\"max_length\", return_tensors=None)\n","        result[\"labels\"] = result[\"input_ids\"].copy()\n","        return result\n","\n","    print(\"Tokenizing...\")\n","    tokenized = ds.map(preprocess, batched=True, remove_columns=ds.column_names)\n","    print(\"✓ Tokenized\\n\")\n","    return tokenized\n","\n","# Train\n","def train(cfg, run_name=None, tags=None):\n","    if run_name is None:\n","        run_name = f\"run-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n","\n","    with mlflow.start_run(run_name=run_name):\n","        print(f\"MLflow Run: {mlflow.active_run().info.run_id}\\n\")\n","        mlflow.log_params(cfg)\n","\n","        # Add tags if provided\n","        if tags:\n","            for key, value in tags.items():\n","                mlflow.set_tag(key, value)\n","\n","        model, tokenizer = load_model(cfg[\"model_name\"])\n","        model = apply_lora(model, cfg)\n","        dataset = load_data(cfg[\"dataset_path\"], tokenizer, cfg[\"max_length\"])\n","\n","        mlflow.log_param(\"dataset_size\", len(dataset))\n","\n","        collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","        args = TrainingArguments(\n","            output_dir=cfg[\"output_dir\"],\n","            per_device_train_batch_size=cfg[\"batch_size\"],\n","            gradient_accumulation_steps=cfg[\"gradient_accumulation_steps\"],\n","            num_train_epochs=cfg[\"num_epochs\"],\n","            learning_rate=cfg[\"learning_rate\"],\n","            fp16=USE_GPU, logging_steps=5, save_strategy=\"epoch\", save_total_limit=2,\n","            optim=\"paged_adamw_8bit\" if USE_GPU else \"adamw_torch\",\n","            warmup_steps=cfg[\"warmup_steps\"], lr_scheduler_type=\"cosine\",\n","            gradient_checkpointing=False,  # Disable to avoid conflicts with PEFT\n","            report_to=\"none\", dataloader_pin_memory=USE_GPU,\n","            remove_unused_columns=False  # Important for PEFT\n","        )\n","\n","        trainer = Trainer(\n","            model=model, args=args, train_dataset=dataset,\n","            data_collator=collator, callbacks=[MLflowCallback()]\n","        )\n","\n","        print(\"=\"*60)\n","        print(\"Training...\")\n","        print(\"=\"*60 + \"\\n\")\n","\n","        result = trainer.train()\n","\n","        mlflow.log_metrics({\n","            \"final_loss\": result.training_loss,\n","            \"train_runtime\": result.metrics[\"train_runtime\"],\n","            \"train_samples_per_second\": result.metrics.get(\"train_samples_per_second\", 0),\n","        })\n","\n","        print(\"\\nSaving...\")\n","        trainer.save_model(cfg[\"output_dir\"])\n","        tokenizer.save_pretrained(cfg[\"output_dir\"])\n","        mlflow.log_artifacts(cfg[\"output_dir\"], artifact_path=\"model\")\n","\n","        run_id = mlflow.active_run().info.run_id\n","\n","        print(\"\\n\" + \"=\"*60)\n","        print(\"✓ Training Complete!\")\n","        print(\"=\"*60)\n","        print(f\"Model: {cfg['output_dir']}\")\n","        print(f\"Run ID: {run_id}\")\n","        print(\"=\"*60 + \"\\n\")\n","\n","        return run_id, model, tokenizer\n","\n","# Retrain function\n","def retrain(base_run_id=None, new_dataset_path=None, config_updates=None):\n","    \"\"\"\n","    Retrain a model with new data or updated configuration\n","\n","    Args:\n","        base_run_id: Previous MLflow run ID (for tracking lineage)\n","        new_dataset_path: Path to new training data\n","        config_updates: Dict of config parameters to update\n","\n","    Returns:\n","        run_id, model, tokenizer\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"RETRAINING MODEL\")\n","    print(\"=\"*60 + \"\\n\")\n","\n","    # Create new config based on updates\n","    retrain_config = CONFIG.copy()\n","\n","    if new_dataset_path:\n","        retrain_config[\"dataset_path\"] = new_dataset_path\n","        print(f\"→ Using new dataset: {new_dataset_path}\")\n","\n","    if config_updates:\n","        retrain_config.update(config_updates)\n","        print(f\"→ Config updates: {config_updates}\")\n","\n","    print()\n","\n","    # Create run name\n","    if base_run_id:\n","        run_name = f\"retrain-{base_run_id[:8]}-{datetime.now().strftime('%H%M%S')}\"\n","    else:\n","        run_name = f\"retrain-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n","\n","    # Prepare tags\n","    tags = {\n","        \"retrained\": \"true\",\n","        \"retrain_timestamp\": datetime.now().isoformat(),\n","    }\n","    if base_run_id:\n","        tags[\"base_run_id\"] = base_run_id\n","\n","    # Train with new config\n","    run_id, model, tokenizer = train(retrain_config, run_name=run_name, tags=tags)\n","\n","    print(\"✓ Retraining complete!\\n\")\n","\n","    return run_id, model, tokenizer\n","\n","# Compare runs function\n","def compare_runs(experiment_name=None):\n","    \"\"\"Compare all runs in the experiment\"\"\"\n","    import pandas as pd\n","\n","    if experiment_name is None:\n","        experiment_name = mlflow.get_experiment_by_name(exp_name).name\n","\n","    experiment = mlflow.get_experiment_by_name(experiment_name)\n","    if not experiment:\n","        print(f\"Experiment '{experiment_name}' not found\")\n","        return None\n","\n","    runs = mlflow.search_runs(\n","        experiment_ids=[experiment.experiment_id],\n","        order_by=[\"start_time DESC\"]\n","    )\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"MODEL COMPARISON\")\n","    print(\"=\"*80)\n","\n","    # Select relevant columns\n","    cols = [\"run_id\", \"start_time\", \"status\",\n","            \"params.learning_rate\", \"params.num_epochs\", \"params.lora_r\",\n","            \"metrics.final_loss\", \"metrics.train_runtime\"]\n","\n","    available_cols = [col for col in cols if col in runs.columns]\n","\n","    if len(runs) > 0:\n","        comparison = runs[available_cols]\n","        print(comparison.to_string(index=False))\n","    else:\n","        print(\"No runs found\")\n","\n","    print(\"=\"*80 + \"\\n\")\n","\n","    return runs\n","\n","# Get best model\n","def get_best_run(metric=\"final_loss\", ascending=True):\n","    \"\"\"Get the best run based on a metric\"\"\"\n","    experiment = mlflow.get_experiment_by_name(exp_name)\n","\n","    order = \"ASC\" if ascending else \"DESC\"\n","    runs = mlflow.search_runs(\n","        experiment_ids=[experiment.experiment_id],\n","        order_by=[f\"metrics.{metric} {order}\"],\n","        max_results=1\n","    )\n","\n","    if len(runs) == 0:\n","        print(\"No runs found\")\n","        return None\n","\n","    best_run = runs.iloc[0]\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(f\"BEST MODEL (by {metric})\")\n","    print(\"=\"*60)\n","    print(f\"Run ID: {best_run['run_id']}\")\n","    print(f\"{metric}: {best_run[f'metrics.{metric}']}\")\n","    print(f\"Learning Rate: {best_run.get('params.learning_rate', 'N/A')}\")\n","    print(f\"Epochs: {best_run.get('params.num_epochs', 'N/A')}\")\n","    print(\"=\"*60 + \"\\n\")\n","\n","    return best_run['run_id']\n","\n","# Test\n","def test(model, tokenizer, prompts=None):\n","    if prompts is None:\n","        prompts = [\"def fibonacci(n):\", \"function sum(arr) {\", \"class Model:\"]\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"TESTING MODEL\")\n","    print(\"=\"*60 + \"\\n\")\n","\n","    for prompt in prompts:\n","        print(f\"Prompt: {prompt}\")\n","        inputs = tokenizer(prompt, return_tensors=\"pt\")\n","        if USE_GPU:\n","            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n","\n","        outputs = model.generate(\n","            **inputs, max_length=150, temperature=0.7,\n","            do_sample=True, pad_token_id=tokenizer.eos_token_id\n","        )\n","        print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","        print(\"-\"*60 + \"\\n\")\n","\n","# ============================================================\n","# MAIN EXECUTION\n","# ============================================================\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"TRAINING INITIAL MODEL\")\n","print(\"=\"*60 + \"\\n\")\n","\n","# Step 1: Train initial model\n","run_id_1, model_1, tokenizer_1 = train(CONFIG)\n","\n","# Step 2: Test initial model\n","#test(model_1, tokenizer_1)\n","\n","# ============================================================\n","# RETRAINING EXAMPLES\n","# ============================================================\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"RETRAINING OPTIONS\")\n","print(\"=\"*60)\n","print(\"\"\"\n","# Example 1: Retrain with new dataset\n","run_id_2, model_2, tokenizer_2 = retrain(\n","    base_run_id=run_id_1,\n","    new_dataset_path=\"new_code_dataset.json\"\n",")\n","\n","# Example 2: Retrain with different hyperparameters\n","run_id_3, model_3, tokenizer_3 = retrain(\n","    base_run_id=run_id_1,\n","    config_updates={\n","        \"num_epochs\": 2,\n","        \"learning_rate\": 1e-4,\n","        \"lora_r\": 8\n","    }\n",")\n","\n","# Example 3: Retrain with both new data and config\n","run_id_4, model_4, tokenizer_4 = retrain(\n","    base_run_id=run_id_1,\n","    new_dataset_path=\"new_data.json\",\n","    config_updates={\"num_epochs\": 5}\n",")\n","\n","# Compare all runs\n","compare_runs()\n","\n","# Get best model\n","best_run_id = get_best_run(metric=\"final_loss\", ascending=True)\n","\"\"\")\n","\n","# Uncomment below to actually run retraining\n","print(\"\\n\" + \"=\"*60)\n","print(\"RETRAINING WITH NEW PARAMETERS\")\n","print(\"=\"*60 + \"\\n\")\n","\n","run_id_2, model_2, tokenizer_2 = retrain(\n","    base_run_id=run_id_1,\n","    config_updates={\n","        \"num_epochs\": 2,\n","        \"learning_rate\": 1e-4\n","    }\n",")\n","#\n","# test(model_2, tokenizer_2)\n","#\n","# # Compare all runs\n","# compare_runs()\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"ALL DONE!\")\n","print(\"=\"*60)\n","print(f\"\\nInitial model saved to: {CONFIG['output_dir']}\")\n","print(f\"Initial run ID: {run_id_1}\")\n","print(\"\\nView results:\")\n","print(\"  mlflow ui --backend-store-uri ./mlruns\")\n","print(\"  http://localhost:5000\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7adb7df0d286496ab6af41f0661b9d65","0a21516eb4354f848e2b2f8636ec302c","1f239c2bf46d45ddae7c465334b886b9","6cd995d9ca4c4a2d969bdd35c4bafe19","56c2d0100a4748d1aae1a7c2722d8d52","de6c98fd193f452595333b7c6993af95","c6d45285a0cd49e79f841390d1a42820","5b8ef255f6a64042a449c4363603f6e7","f7546600a695470db38d202427d2dc58","253b542ca8e543509ed9a792e32ebba5","3454c1e3a26d433493f5ffd7452c65e3","ee6ea39fde3547c9a32fdd702368f662","e4956e2273b144239c2d1d37d67f7873","5ad4c17d7f6f4ccd950a8b639141c4c8","56377646990340dc910250efb14e1fe9","5e70bdb47fc5456d8b4f46df11bad954","c5aed33ec33b4f56be7ebf955ef1ce6b","f1fcebb450a6497e8c394315d9d30ace","d5db7f09f6d14ed48d691abd0ba8e096","d2c5f2d0be834a168b5975dc1a234a3e","7ba187c8f304428d9cdf728e5977be4a","c43f16b790a84b7da9d4935f33ba08e1"]},"id":"I2UsZQB4GHu0","executionInfo":{"status":"ok","timestamp":1764045075091,"user_tz":300,"elapsed":497711,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}},"outputId":"0358d46e-6ffe-4bf7-e0a0-871ab00e92fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✓ Cleaned directories\n","\n","Installing packages...\n","✓ Packages installed\n","\n","✓ Imports complete\n","\n","✓ MLflow experiment: starcoder-20251125042334\n","\n","✓ GPU: Tesla T4\n","\n","\n","============================================================\n","TRAINING INITIAL MODEL\n","============================================================\n","\n","MLflow Run: c1731b7e6b3f4c828b8a816f3d87f5be\n","\n","Loading /content/drive/MyDrive//starcoder2-3b...\n","✓ Model loaded\n","\n","\n","trainable params: 9,093,120 || all params: 3,039,464,448 || trainable%: 0.2992\n","\n","Loading /content/drive/MyDrive/code_dataset.json...\n"]},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7adb7df0d286496ab6af41f0661b9d65"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✓ 42 examples\n","\n","Tokenizing...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/42 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee6ea39fde3547c9a32fdd702368f662"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✓ Tokenized\n","\n","============================================================\n","Training...\n","============================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9/9 01:16, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>2.462300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Saving...\n","\n","============================================================\n","✓ Training Complete!\n","============================================================\n","Model: ./starcoder-finetuned\n","Run ID: c1731b7e6b3f4c828b8a816f3d87f5be\n","============================================================\n","\n","\n","============================================================\n","RETRAINING OPTIONS\n","============================================================\n","\n","# Example 1: Retrain with new dataset\n","run_id_2, model_2, tokenizer_2 = retrain(\n","    base_run_id=run_id_1,\n","    new_dataset_path=\"new_code_dataset.json\"\n",")\n","\n","# Example 2: Retrain with different hyperparameters\n","run_id_3, model_3, tokenizer_3 = retrain(\n","    base_run_id=run_id_1,\n","    config_updates={\n","        \"num_epochs\": 2,\n","        \"learning_rate\": 1e-4,\n","        \"lora_r\": 8\n","    }\n",")\n","\n","# Example 3: Retrain with both new data and config\n","run_id_4, model_4, tokenizer_4 = retrain(\n","    base_run_id=run_id_1,\n","    new_dataset_path=\"new_data.json\",\n","    config_updates={\"num_epochs\": 5}\n",")\n","\n","# Compare all runs\n","compare_runs()\n","\n","# Get best model\n","best_run_id = get_best_run(metric=\"final_loss\", ascending=True)\n","\n","\n","============================================================\n","RETRAINING WITH NEW PARAMETERS\n","============================================================\n","\n","\n","============================================================\n","RETRAINING MODEL\n","============================================================\n","\n","→ Config updates: {'num_epochs': 2, 'learning_rate': 0.0001}\n","\n","MLflow Run: 2c057b3a65b442c3ad175ef21e2ff19e\n","\n","Loading /content/drive/MyDrive//starcoder2-3b...\n","✓ Model loaded\n","\n","\n","trainable params: 9,093,120 || all params: 3,039,464,448 || trainable%: 0.2992\n","\n","Loading /content/drive/MyDrive/code_dataset.json...\n","✓ 42 examples\n","\n","Tokenizing...\n","✓ Tokenized\n","\n","============================================================\n","Training...\n","============================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6/6 00:48, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>2.462600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Saving...\n","\n","============================================================\n","✓ Training Complete!\n","============================================================\n","Model: ./starcoder-finetuned\n","Run ID: 2c057b3a65b442c3ad175ef21e2ff19e\n","============================================================\n","\n","✓ Retraining complete!\n","\n","\n","============================================================\n","ALL DONE!\n","============================================================\n","\n","Initial model saved to: ./starcoder-finetuned\n","Initial run ID: c1731b7e6b3f4c828b8a816f3d87f5be\n","\n","View results:\n","  mlflow ui --backend-store-uri ./mlruns\n","  http://localhost:5000\n","============================================================\n"]}]}]}