{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aJWQUp82Nzn0jttZJ8o8CnvnoBSrG1-q","timestamp":1764047063269}],"gpuType":"T4","mount_file_id":"1aJWQUp82Nzn0jttZJ8o8CnvnoBSrG1-q","authorship_tag":"ABX9TyOFqBWs4Au+U1rUipxrG26Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ============================================================\n","# Run FastAPI Inference Server in Google Colab with ngrok\n","# Makes your model accessible via public URL\n","# ============================================================\n","\n","# Install dependencies\n","!pip install -q fastapi uvicorn pyngrok nest-asyncio\n","\n","import nest_asyncio\n","from pyngrok import ngrok\n","import uvicorn\n","from fastapi import FastAPI, HTTPException\n","from pydantic import BaseModel\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel\n","import time\n","from typing import List"],"metadata":{"id":"ABzdPHYDUnvB","executionInfo":{"status":"ok","timestamp":1764046641519,"user_tz":300,"elapsed":53891,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Allow nested async (required for Colab)\n","nest_asyncio.apply()\n","\n","# ============================================================\n","# MODELS\n","# ============================================================\n","class GenerateRequest(BaseModel):\n","    prompt: str\n","    max_length: int = 150\n","    temperature: float = 0.7\n","    do_sample: bool = True\n","\n","class GenerateResponse(BaseModel):\n","    generated_text: str\n","    inference_time: float"],"metadata":{"id":"gS4NgeSWUw9l","executionInfo":{"status":"ok","timestamp":1764046648228,"user_tz":300,"elapsed":55,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# SETUP\n","# ============================================================\n","app = FastAPI(title=\"StarCoder2 API\")\n","\n","# Global variables\n","MODEL = None\n","TOKENIZER = None\n","USE_GPU = torch.cuda.is_available()\n","\n","print(f\"GPU Available: {USE_GPU}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVvIuscOUzHP","executionInfo":{"status":"ok","timestamp":1764046651928,"user_tz":300,"elapsed":39,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}},"outputId":"5334407f-e70a-414b-d6d4-d7c688d741e6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU Available: True\n"]}]},{"cell_type":"code","source":["def load_model():\n","    global MODEL, TOKENIZER\n","\n","    print(\"Loading model...\")\n","\n","    BASE_MODEL = \"/content/drive/MyDrive/starcoder2-3b\"\n","    LORA_PATH = \"/content/starcoder-finetuned\"\n","\n","    # Tokenizer\n","    TOKENIZER = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n","    TOKENIZER.pad_token = TOKENIZER.eos_token\n","\n","    # Model\n","    if USE_GPU:\n","        base_model = AutoModelForCausalLM.from_pretrained(\n","            BASE_MODEL,\n","            torch_dtype=torch.float16,\n","            device_map=\"auto\",\n","            trust_remote_code=True\n","        )\n","    else:\n","        base_model = AutoModelForCausalLM.from_pretrained(\n","            BASE_MODEL,\n","            torch_dtype=torch.float32,\n","            trust_remote_code=True\n","        )\n","\n","    # Load LoRA and merge for faster inference\n","    print(\"Merging LoRA weights...\")\n","    MODEL = PeftModel.from_pretrained(base_model, LORA_PATH)\n","    MODEL = MODEL.merge_and_unload()\n","    MODEL.eval()\n","\n","    print(\"✓ Model loaded!\")"],"metadata":{"id":"KB1wdtQ_U3OZ","executionInfo":{"status":"ok","timestamp":1764046744745,"user_tz":300,"elapsed":44,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","# ============================================================\n","# ENDPOINTS\n","# ============================================================\n","@app.get(\"/\")\n","async def root():\n","    return {\n","        \"message\": \"StarCoder2 Inference API\",\n","        \"endpoints\": [\"/generate\", \"/health\"],\n","        \"model\": \"starcoder2-3b-finetuned\"\n","    }\n","\n","@app.get(\"/health\")\n","async def health():\n","    return {\n","        \"status\": \"healthy\",\n","        \"model_loaded\": MODEL is not None,\n","        \"gpu\": USE_GPU\n","    }\n","\n","@app.post(\"/generate\", response_model=GenerateResponse)\n","async def generate(request: GenerateRequest):\n","    if MODEL is None:\n","        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n","\n","    start = time.time()\n","\n","    try:\n","        inputs = TOKENIZER(request.prompt, return_tensors=\"pt\")\n","\n","        if USE_GPU:\n","            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n","\n","        with torch.no_grad():\n","            outputs = MODEL.generate(\n","                **inputs,\n","                max_length=request.max_length,\n","                temperature=request.temperature if request.do_sample else 1.0,\n","                do_sample=request.do_sample,\n","                pad_token_id=TOKENIZER.eos_token_id,\n","                use_cache=True,\n","            )\n","\n","        text = TOKENIZER.decode(outputs[0], skip_special_tokens=True)\n","        elapsed = time.time() - start\n","\n","        return GenerateResponse(generated_text=text, inference_time=elapsed)\n","\n","    except Exception as e:\n","        raise HTTPException(status_code=500, detail=str(e))"],"metadata":{"id":"0M5RdjZDUjSC","executionInfo":{"status":"ok","timestamp":1764046700981,"user_tz":300,"elapsed":36,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# START SERVER\n","# ============================================================\n","# Load model first\n","load_model()\n","\n","# Setup ngrok tunnel (get free auth token from ngrok.com)\n","print(\"\\n\" + \"=\"*60)\n","print(\"Setting up ngrok tunnel...\")\n","print(\"=\"*60)\n","\n","# Optional: Set your ngrok auth token\n","ngrok.set_auth_token(\"35xJxc60HL0DZahzW4SfIwyq7nK_6KHx5x9m9w7MWtngkgdMz\")\n","\n","# Start ngrok tunnel\n","public_url = ngrok.connect(8000)\n","print(f\"\\n✓ Public URL: {public_url}\")\n","print(f\"\\nAPI Docs: {public_url}/docs\")\n","print(\"=\"*60 + \"\\n\")\n","\n","# Start server\n","print(\"Starting FastAPI server...\")\n","print(\"Press Ctrl+C to stop\\n\")\n","\n","uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":599},"id":"PiVc9yK_VDnd","executionInfo":{"status":"error","timestamp":1764046817699,"user_tz":300,"elapsed":67762,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}},"outputId":"3ed791a9-365e-4392-f42b-49c980137e30"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model...\n","Merging LoRA weights...\n","✓ Model loaded!\n","\n","============================================================\n","Setting up ngrok tunnel...\n","============================================================\n","\n","✓ Public URL: NgrokTunnel: \"https://capitalistic-subfusiform-iluminada.ngrok-free.dev\" -> \"http://localhost:8000\"\n","\n","API Docs: NgrokTunnel: \"https://capitalistic-subfusiform-iluminada.ngrok-free.dev\" -> \"http://localhost:8000\"/docs\n","============================================================\n","\n","Starting FastAPI server...\n","Press Ctrl+C to stop\n","\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"asyncio.run() cannot be called from a running event loop","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1155742763.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Press Ctrl+C to stop\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0muvicorn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0.0.0.0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"info\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/uvicorn/main.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, timeout_worker_healthcheck, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mMultiprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# pragma: full coverage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, sockets)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masyncio_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loop_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug, loop_factory)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# fail fast with short traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    192\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"]}]},{"cell_type":"code","source":["# ============================================================\n","# USAGE FROM ANYWHERE\n","# ============================================================\n","import requests\n","\n","# Your public URL from ngrok\n","API_URL = \"https://xxxx-xx-xxx-xxx-xx.ngrok-free.app\"\n","\n","# Generate code\n","response = requests.post(\n","    f\"{API_URL}/generate\",\n","    json={\n","        \"prompt\": \"def fibonacci(n):\",\n","        \"max_length\": 150,\n","        \"temperature\": 0.7\n","    }\n",")\n","\n","result = response.json()\n","print(f\"Generated: {result['generated_text']}\")\n","print(f\"Time: {result['inference_time']}s\")"],"metadata":{"id":"VgRFz_hnVJUX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# Simple Fast Inference in Colab (No Server Setup)\n","# Just run this cell and use the functions directly\n","# ============================================================\n","\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel\n","import time\n","\n","print(\"=\"*60)\n","print(\"LOADING MODEL FOR FAST INFERENCE\")\n","print(\"=\"*60 + \"\\n\")\n","\n","# Configuration\n","BASE_MODEL = \"/content/drive/MyDrive/starcoder2-3b\"\n","LORA_PATH = \"/content/starcoder-finetuned\"\n","\n","# Check GPU\n","USE_GPU = torch.cuda.is_available()\n","print(f\"Using: {'GPU ✓' if USE_GPU else 'CPU ⚠'}\\n\")\n","\n","# Load tokenizer\n","print(\"Loading tokenizer...\")\n","tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Load model\n","print(\"Loading model...\")\n","if USE_GPU:\n","    base_model = AutoModelForCausalLM.from_pretrained(\n","        BASE_MODEL,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\",\n","        trust_remote_code=True,\n","        low_cpu_mem_usage=True\n","    )\n","else:\n","    base_model = AutoModelForCausalLM.from_pretrained(\n","        BASE_MODEL,\n","        torch_dtype=torch.float32,\n","        trust_remote_code=True,\n","        low_cpu_mem_usage=True\n","    )\n","\n","# Load LoRA weights\n","print(\"Loading fine-tuned weights...\")\n","model = PeftModel.from_pretrained(base_model, LORA_PATH)\n","\n","# CRITICAL: Merge LoRA weights for faster inference\n","print(\"Merging weights (this makes inference much faster)...\")\n","model = model.merge_and_unload()\n","model.eval()\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"✓ MODEL READY FOR FAST INFERENCE!\")\n","print(\"=\"*60 + \"\\n\")\n","\n","# ============================================================\n","# INFERENCE FUNCTIONS\n","# ============================================================\n","\n","def generate(prompt, max_length=150, temperature=0.7, fast=True):\n","    \"\"\"\n","    Generate code from a prompt\n","\n","    Args:\n","        prompt: Code prompt\n","        max_length: Maximum length of generated code\n","        temperature: Sampling temperature (ignored if fast=True)\n","        fast: Use fast greedy decoding (recommended)\n","\n","    Returns:\n","        Generated code as string\n","    \"\"\"\n","    start_time = time.time()\n","\n","    # Tokenize\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    if USE_GPU:\n","        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n","\n","    # Generate\n","    with torch.no_grad():\n","        if fast:\n","            # Fast mode: greedy decoding, no sampling\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=max_length,\n","                do_sample=False,\n","                pad_token_id=tokenizer.eos_token_id,\n","                use_cache=True,\n","            )\n","        else:\n","            # Quality mode: sampling enabled\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=max_length,\n","                temperature=temperature,\n","                do_sample=True,\n","                top_p=0.95,\n","                pad_token_id=tokenizer.eos_token_id,\n","                use_cache=True,\n","            )\n","\n","    # Decode\n","    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    elapsed = time.time() - start_time\n","\n","    print(f\"⏱️  Generation time: {elapsed:.2f}s\")\n","    return result\n","\n","def generate_batch(prompts, max_length=150, fast=True):\n","    \"\"\"\n","    Generate code for multiple prompts at once (faster!)\n","\n","    Args:\n","        prompts: List of code prompts\n","        max_length: Maximum length for each generation\n","        fast: Use fast greedy decoding\n","\n","    Returns:\n","        List of generated code strings\n","    \"\"\"\n","    start_time = time.time()\n","\n","    # Tokenize all prompts\n","    inputs = tokenizer(\n","        prompts,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True\n","    )\n","    if USE_GPU:\n","        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n","\n","    # Generate batch\n","    with torch.no_grad():\n","        if fast:\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=max_length,\n","                do_sample=False,\n","                pad_token_id=tokenizer.eos_token_id,\n","                use_cache=True,\n","            )\n","        else:\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=max_length,\n","                temperature=0.7,\n","                do_sample=True,\n","                top_p=0.95,\n","                pad_token_id=tokenizer.eos_token_id,\n","                use_cache=True,\n","            )\n","\n","    # Decode all\n","    results = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n","    elapsed = time.time() - start_time\n","\n","    print(f\"⏱️  Batch generation time: {elapsed:.2f}s\")\n","    print(f\"⏱️  Average per prompt: {elapsed/len(prompts):.2f}s\")\n","    return results\n","\n","# ============================================================\n","# EXAMPLES\n","# ============================================================\n","\n","print(\"=\"*60)\n","print(\"EXAMPLE USAGE\")\n","print(\"=\"*60 + \"\\n\")\n","\n","# Example 1: Single generation\n","print(\"Example 1: Single generation\")\n","print(\"-\" * 60)\n","prompt1 = \"def fibonacci(n):\"\n","print(f\"Prompt: {prompt1}\\n\")\n","result1 = generate(prompt1, max_length=150)\n","print(f\"\\nGenerated:\\n{result1}\")\n","print(\"=\"*60 + \"\\n\")\n","\n","# Example 2: Batch generation (faster for multiple prompts)\n","print(\"Example 2: Batch generation\")\n","print(\"-\" * 60)\n","prompts = [\n","    \"def calculate_sum(numbers):\",\n","    \"function sortArray(arr) {\",\n","    \"class DatabaseManager:\",\n","]\n","print(f\"Generating {len(prompts)} prompts...\\n\")\n","results = generate_batch(prompts, max_length=120)\n","\n","for i, (prompt, result) in enumerate(zip(prompts, results), 1):\n","    print(f\"\\n{i}. Prompt: {prompt}\")\n","    print(f\"   Result: {result[:80]}...\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"✓ ALL DONE!\")\n","print(\"=\"*60)\n","print(\"\"\"\n","Now you can use:\n","  - generate(prompt)           # Single generation\n","  - generate_batch([prompts])  # Batch generation (faster)\n","\n","Examples:\n","  result = generate(\"def quicksort(arr):\")\n","  results = generate_batch([\"def func1():\", \"def func2():\"])\n","\"\"\")\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yxec2g_CY-bU","executionInfo":{"status":"ok","timestamp":1764047024020,"user_tz":300,"elapsed":70332,"user":{"displayName":"Aparna Shree","userId":"14693552059682037054"}},"outputId":"2b71a23c-0bed-4459-f5f7-f09d4d612fe7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","LOADING MODEL FOR FAST INFERENCE\n","============================================================\n","\n","Using: GPU ✓\n","\n","Loading tokenizer...\n","Loading model...\n","Loading fine-tuned weights...\n","Merging weights (this makes inference much faster)...\n","\n","============================================================\n","✓ MODEL READY FOR FAST INFERENCE!\n","============================================================\n","\n","============================================================\n","EXAMPLE USAGE\n","============================================================\n","\n","Example 1: Single generation\n","------------------------------------------------------------\n","Prompt: def fibonacci(n):\n","\n"]},{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["⏱️  Generation time: 6.26s\n","\n","Generated:\n","def fibonacci(n):\n","    if n == 0:\n","        return 0\n","    elif n == 1:\n","        return 1\n","    else:\n","        return fibonacci(n-1) + fibonacci(n-2)\n","\n","print(fibonacci(10))\n","\n","# +\n","# 10. Write a Python program to find the sum of the first n positive integers.\n","# -\n","\n","def sum_of_first_n_positive_integers(n):\n","    if n == 0:\n","        return 0\n","    else:\n","        return n + sum_of_first_n_positive_integers(n-1)\n","print(sum_of_first_n_\n","============================================================\n","\n","Example 2: Batch generation\n","------------------------------------------------------------\n","Generating 3 prompts...\n","\n","⏱️  Batch generation time: 4.66s\n","⏱️  Average per prompt: 1.55s\n","\n","1. Prompt: def calculate_sum(numbers):\n","   Result: def calculate_sum(numbers):\n","    return sum(numbers)\n","\n","def calculate_product(numbe...\n","\n","2. Prompt: function sortArray(arr) {\n","   Result: function sortArray(arr) {\n","  // sort the array\n","  arr.sort(function(a, b) {\n","    re...\n","\n","3. Prompt: class DatabaseManager:\n","   Result: class DatabaseManager:\n","\tdef __init__(self):\n","\t\tself.db = sqlite3.connect('databas...\n","\n","============================================================\n","✓ ALL DONE!\n","============================================================\n","\n","Now you can use:\n","  - generate(prompt)           # Single generation\n","  - generate_batch([prompts])  # Batch generation (faster)\n","\n","Examples:\n","  result = generate(\"def quicksort(arr):\")\n","  results = generate_batch([\"def func1():\", \"def func2():\"])\n","\n","============================================================\n"]}]}]}