# CustomLLM Inference API Environment Configuration
# Copy this file to .env and update the values according to your setup

# === REQUIRED CONFIGURATION ===
# API authentication key - MUST be set for security
API_KEY=your-secret-api-key-here

# === MLflow Configuration ===
# MLflow tracking server URI
MLFLOW_TRACKING_URI=http://localhost:5000

# Model name in MLflow registry
MODEL_NAME=custom-llm

# Model stage to load (Production, Staging, etc.)
MODEL_STAGE=Production

# === API Server Configuration ===
# Host to bind the API server
API_HOST=0.0.0.0

# Port for the API server
API_PORT=8000

# Number of worker processes (set to 1 for single GPU)
API_WORKERS=1

# === GPU Configuration ===
# CUDA device ID to use
CUDA_VISIBLE_DEVICES=0

# CUDA architecture list for compilation
TORCH_CUDA_ARCH_LIST="6.0;6.1;7.0;7.5;8.0;8.6"

# === Model Configuration ===
# Maximum tokens for input prompts
MAX_INPUT_TOKENS=2048

# Default maximum tokens to generate
DEFAULT_MAX_LENGTH=100

# Default temperature for generation
DEFAULT_TEMPERATURE=0.7

# Default top-p for nucleus sampling
DEFAULT_TOP_P=0.95

# Batch size for GPU inference
INFERENCE_BATCH_SIZE=4

# Timeout for inference requests (seconds)
INFERENCE_TIMEOUT=60

# === Monitoring Configuration ===
# Enable Prometheus metrics
ENABLE_METRICS=true

# Metrics collection directory for multiprocess
PROMETHEUS_MULTIPROC_DIR=/tmp

# === Logging Configuration ===
# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Log format
LOG_FORMAT="%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# === Cache Configuration ===
# Model cache directory
MODEL_CACHE_DIR=/app/.cache

# Enable model caching
ENABLE_MODEL_CACHE=true

# === Security Configuration ===
# CORS allowed origins (comma-separated)
CORS_ORIGINS=*

# Trusted hosts (comma-separated)
TRUSTED_HOSTS=*

# Maximum request size (bytes)
MAX_REQUEST_SIZE=10485760

# === Development Configuration ===
# Enable auto-reload for development
RELOAD=false

# Enable debug mode
DEBUG=false

# === Database Configuration (if using external DB) ===
# Database URL for MLflow backend store
# DATABASE_URL=sqlite:///mlflow.db

# === Redis Configuration (if using Redis for caching) ===
# REDIS_URL=redis://localhost:6379/0

# === Advanced Configuration ===
# Number of threads for tokenization
TOKENIZER_THREADS=4

# Enable mixed precision training
ENABLE_MIXED_PRECISION=true

# Memory fraction for GPU
GPU_MEMORY_FRACTION=0.8

# Enable gradient checkpointing
ENABLE_GRADIENT_CHECKPOINTING=true

# === Health Check Configuration ===
# Health check interval (seconds)
HEALTH_CHECK_INTERVAL=30

# Health check timeout (seconds) 
HEALTH_CHECK_TIMEOUT=10

# === Backup Configuration ===
# Enable automatic model backup
ENABLE_MODEL_BACKUP=false

# Backup directory
BACKUP_DIR=/app/backups

# Backup retention days
BACKUP_RETENTION_DAYS=7