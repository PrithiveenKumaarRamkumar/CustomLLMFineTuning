# Serving Configuration for LLM Inference API
api:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  title: "CustomLLM Inference API"
  description: "Production-ready LLM inference service with FastAPI"
  version: "1.0.0"
  
# Model Configuration
model:
  max_tokens: 2048
  default_max_length: 100
  default_temperature: 0.7
  default_top_p: 0.95
  batch_size: 4
  timeout: 60  # seconds

# MLflow Configuration
mlflow:
  tracking_uri: ${MLFLOW_TRACKING_URI:-http://localhost:5000}
  model_name: ${MODEL_NAME:-custom-llm}
  model_stage: "Production"

# GPU Configuration
gpu:
  memory_fraction: 0.8
  allow_growth: true
  device_id: 0

# Monitoring Configuration
monitoring:
  enable_metrics: true
  metrics_path: "/metrics"
  
# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
# Security Configuration
security:
  api_key_header: "Authorization"
  api_key_scheme: "Bearer"
